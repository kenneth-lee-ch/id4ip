# Import packages
import os
from GraphSurgery import GraphSurgeryClassifier
from id4ip import ID4IP
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import pyAgrum as gum
import numpy as np
from GraphModel import * 
import pyAgrum.lib.image as gumimage
import pandas as pd
import func_timeout
from utils import *
import matplotlib
font = {'weight' : 'bold',
        'size'   : 20}
matplotlib.rc('font', **font)

def run_function(f, list_of_arguments, max_wait, default_value):
    """
        run a function with time restriction on the program. It will exit out of the function once the subprocess is finished and the time is up.

        Arg:
            f: function
            list_of_arguments (list): a list of arguments that should be used by the function f
            max_wait (int): an integer to determine the time restrcition
            default_vaue (None): when nothing is returned, we return None
    """
    try:
        return func_timeout.func_timeout(max_wait, f, args=list_of_arguments)
    except func_timeout.FunctionTimedOut:
        pass
    return default_value

def splitSamplesToXY(graphModel,
                     path_for_sample, 
                     path_for_covariates, 
                     path_for_target,
                     isTestSample=False,
                     saveFiles=True):
    """
        take the sample generated by Pyagrum and split them into features and target

        Arg:
            graphModel(GraphModel object): an object generated by the GraphModel class 
            path_for_sample (str): a filepath to get the training/test file
            path_for_covariates (str): a filepath to save the feature columns dataframe
            path_for_target (str): a filepath to save the target column
            isTestSample (bool): determine whether we are splitting the test sample
            saveFiles (bool): determine whether we need to save the feature columns and targets as files

    """
    X_col = graphModel.graphnames[:]
    X_col.remove(graphModel.target)
    Y_col = [graphModel.target]
    # ready the training data with respect to ith graph
    df = pd.read_csv(path_for_sample)
    X  = df[X_col] # get the features
    y = df[Y_col] # get the target
    if saveFiles:
        X.to_csv(path_for_covariates)
        y.to_csv(path_for_target)
    if isTestSample:
        X_all_var_including_U = df.drop(Y_col, axis=1)
        return X, y, X_all_var_including_U
    else:
        return X, y



def plot_result(result_df, num_node, time_restriction, gt_loss, filepath=None, savefig=True):
    """
        plot the result for the result of this experiment
        Arg:
            result_df (df): a dataframe that records all the training losses and test losses from each estimator class
            num_node (int):  the number of variables on the graph that is generated for the experiment
            time_restriction (int): number of seconds for time restriction 
            gt_loss (float): a zero and one loss computed for P(target|Pa(target)) 
            filepath (str): a filepath to save the output image 
            savefig (bool): determine whether an image file should be saved
    """
    # get the df with respect to each algorithm
    gse_df =result_df[result_df["Algorithm"]=="GSE"]
    id4ip_df = result_df[result_df["Algorithm"]=="ID4IP"]
    # plot GSE result
    plt.plot(gse_df["Runtime, sec"].tolist(), gse_df["Test Loss"].tolist(),  marker = "h", linestyle = "solid", color = "m", label="n=" +str(num_node)+", GSE")
    # plot id4ip 
    plt.plot(id4ip_df["Runtime, sec"].tolist(), id4ip_df["Test Loss"].tolist(), marker = "*", linestyle= "dashed",color = "c", label="n=" +str(num_node)+ ", ID4IP")
    # axis labels
    plt.xlabel('Runtime, sec', fontweight = "bold")
    plt.ylabel('Test Loss', fontweight = "bold")
    plt.ylim(0, 0.51)
    #plt.yticks(np.arange(0, 0.6, 0.05))
    plt.xticks(np.arange(0, time_restriction+1, 100))
    # add red color 
    plt.axhline(y=0.5, color='red', label="Worse Case")
    plt.axhline(y=gt_loss, color='green', label="P(Y|Pa(Y))")
    # show the legend
    plt.legend(loc='lower left', prop={'size': 9})
    plt.gcf().subplots_adjust(bottom=0.20)
    if savefig:
        plt.savefig(filepath, dpi=300, bbox_inches="tight")
    plt.close()


def getGroundTruthLoss(Xtest_with_U, ytest, target, idToName, bn, loss_func):
    """
        compute the loss for P(target|Pa(target))

        Arg:
            Xtest_with_U (df): a dataframe that represents testing data, including the latent variables in the columns 
            ytest (df): 
            target (str): a target name in string
            idToName (dict): a dictionary that maps variable ID from the PyAgrum BayesNet object to their string names
            bn (PyArgum BayesNet object): an object generated by PyAgrum BayesNet class
            loss_func (function): a loss function
    """
    parents_ids = bn.parents(target)
    conditioning_set = [idToName[id] for id in parents_ids] # convert id to string names
    ls_of_combination = binary_combinations(len(conditioning_set))
    Xtest_with_U_copy = Xtest_with_U.copy()
    Xtest_with_U_copy.loc[:,target] = None
    for counter, i in enumerate(ls_of_combination):
        evidence = dict(zip(conditioning_set, i))
        p = gum.getPosterior(bn, evs= evidence, target=target)
        # get the correspond value according to the max
        pred_val = p.argmax()[0][0][target]
        mask = (Xtest_with_U_copy[conditioning_set]==i).all(axis=1)
        Xtest_with_U_copy.loc[mask, target] = pred_val
    pred = Xtest_with_U_copy[[target]]
    test_loss = loss_func(ytest, pred)
    return test_loss


def eval(estimators, Xtrain,  ytrain, Xtest, ytest, loss_func):
    """
        a pipeline that takes all the queries updated at each time instance and compile them into a dataframe
        Arg:
            estimators (tuple) : a tuple that contains a string and an estimator class
            Xtrain (df): a dataframe that contains the features for training
            ytrain (df): a dataframe that contains only the target for training
            Xtest (df): a dataframe that contains the features for testing
            ytest (df): a dataframe that contains only the target for testing
            loss_func (function): a loss function
    """
    dfs = []
    for estimator_tuple in estimators:
        estimator_name, estimator = estimator_tuple
        print("Fitting {}...".format(estimator_name))
        estimator.fit(Xtrain,  ytrain, loss_func, config["time_restriction"])
        print("----Finished------")
        # get the test losses from all causal queries found for the estimator
        test_losses = []
        time_instances = []
        if not estimator.id_results:
            # if the estimator returns an empty list, we treat that as 0.5 test loss
            test_losses.append(0.5)
            time_instances.append(config["time_restriction"])
        else:
            for id_result in estimator.id_results:
                cpt = id_result.eval()
                y_pred = estimator.predict(Xtest, cpt)
                test_loss = loss_func(ytest, y_pred)
                test_losses.append(test_loss)
            time_instances = estimator.time_instances
            # Ensure the length of time instances and the losses are the same
        if len(test_losses) != len(estimator.time_instances):
            # we pad the losses
            length_diff = len(estimator.time_instances) - len(test_losses)
            none_ls =  [None for _ in range(length_diff)]
            test_losses =  test_losses + none_ls
          
        df = pd.DataFrame({"Runtime, sec": time_instances, "Test Loss": test_losses, "Train Loss": estimator.train_losses})
        df["Algorithm"] = estimator_name
        dfs.append(df)
    # concate all dfs 
    return pd.concat(dfs)



def main(**config):
    """
        run the pipeline of the experiment
    """
    path = config["parent_directory"]
    # Check whether the specified path exists or not
    isExist = os.path.exists(path)
    if not isExist:
        # Create a new directory because it does not exist
        os.makedirs(path)
        print("The new directory is created!")

    # instantiate a graph model 
    g = GraphModel(**config)

    # generate training sample
    train_data_output_filepath = g.parent_directory + "train_sampleFromBN_n" + str(g.num_nodes) + ".csv"
    g.generateTrainingSample(config["usePerfectDist"], train_data_output_filepath)
    if g.bn_without_latent is None:
        print("Fail to learn the probability distribution from data. Exit out of the program. Please increase the number of training samples.")
        return None, None
    print("Picked the target:{}".format(g.target))
    
    
    # split the training data
    Xtrain__output_filepath = g.parent_directory + "X_train_n_" + str(g.num_nodes) + ".csv"
    ytrain_output_filepath = g.parent_directory + "y_train_n_" + str(g.num_nodes) + ".csv"
    X_train, y_train = splitSamplesToXY(g,
                                      path_for_sample=train_data_output_filepath, 
                                      path_for_covariates= Xtrain__output_filepath, 
                                      path_for_target=ytrain_output_filepath, 
                                      isTestSample= False,
                                      saveFiles=config["output_train_test_data"])
        
    
    ### Testing phrase

    # Add a selection variable
    g.addSelectionVariable()

    # generate test sample
    bn_for_generating_test_samples = gum.BayesNet(g.bn_with_latent_as_observed_without_S) # make a copy
    gum.initRandom(config['sampleseed'])
    for chs in g.chS:
        bn_for_generating_test_samples.generateCPT(chs) 
    test_data_output_filepath = g.parent_directory + "test_sampleFromBN_n" + str(g.num_nodes) + ".csv"
    gum.initRandom(config['sampleseed'])
    gum.generateSample(bn_for_generating_test_samples, g.test_sample_size, name_out=test_data_output_filepath)

    # convert bn to causal models
    g.convertBayesNetToCausalModel()

    # split the testing data after a variable being intervened by S
    Xtest__output_filepath = g.parent_directory + "X_test_n_" + str(g.num_nodes) + ".csv"
    ytest_output_filepath = g.parent_directory + "y_test_n_" + str(g.num_nodes) + ".csv"
    Xtest_without_U, ytest, Xtest_with_U = splitSamplesToXY(g, 
                                                            path_for_sample = test_data_output_filepath,
                                                            path_for_covariates= Xtest__output_filepath, 
                                                            path_for_target=ytest_output_filepath, 
                                                            isTestSample= True,
                                                            saveFiles=config["output_train_test_data"])
        
    # output graph image
    if config["output_graph_image"]:
        image_filepath = g.parent_directory + "graph_n"+ str(g.num_nodes) + ".png"
        # generate the image based on causal model that has a selection variable
        gumimage.export(g.cm_with_S, image_filepath)
        print("Graph generated!")
        
    # fit all estimators with training data
    estimators = [
                  ("ID4IP", ID4IP(g)),
                  ("GSE", GraphSurgeryClassifier(g))
                  ]
    
    result_df = eval(estimators, X_train, y_train, Xtest_without_U, ytest, zero_one_loss)
    print("-----Comparison-------")
    print(result_df)
    # get the loss for ground truth
    gt_loss = getGroundTruthLoss(Xtest_with_U, ytest, g.target, g.idToName, g.bn_with_latent_as_observed_without_S, zero_one_loss)
    print("Ground Truth Test Loss:{}".format(gt_loss))
    # apply logistic regression in the end using all features
    
    return result_df, gt_loss



if __name__ == "__main__":
    # specify the configuration
    n = [16, 25, 32]
    for i in range(len(n)):
        config = {
            "num_nodes": n[i], # number of observed variables
            "num_latent": int(n[i]/2), # number of unobserved variables, must be greater than 0
            "time_restriction": 600, # number of seconds to restrict program execution
            'arc_ratio': 3,  # 
            "parent_directory": "time_vs_loss_results/", # root filepath to save output files
            "train_sample_size": 100000,  # number of samples generated from the true DAG 
            "test_sample_size": 10000,
            "graphseed": 2023, # random seed
            "sampleseed": 2023,
            "output_train_test_data": True,
            "output_graph_image": True,
            "usePerfectDist": True, # if false, it uses pyagrum to learn the distribution from data
            "user_provided_bayesNet": None, # provide a graph where everything is observed, including the latents you want to specify
            "user_provided_target": None, # # specify the target if "user_provided_graph" is provided
            "user_provided_latent_spec": None # specify the latent specification if "user_provided_graph" is provided
        }

        # The following is an example for using a graph provided by the user 
        # bn = gum.fastBN("Z->X->Y; U->Z; U->Y")
        # latent_spec = [("U", ("Z", "Y"))] 
        #
        # config = {
        #     "num_nodes": 3, 
        #     "num_latent": 1, 
        #     "time_restriction": 600, 
        #     'arc_ratio': 1,   
        #     "parent_directory": "results/", 
        #     "train_sample_size": 100000,  
        #     "test_sample_size": 10000,
        #     "seed": 10, # random seed
        #     "output_train_test_data": True,
        #     "output_graph_image": True,
        #     "usePerfectDist": True, 
        #     "user_provided_bayesNet": bn, 
        #     "user_provided_target": "Y", 
        #     "user_provided_latent_spec": latent_spec
        # }

        # call on experiment
        res_df, gt_loss = main(**config)

        # save the predicted result
        res_df.to_csv(config["parent_directory"] + "n_" + str(config["num_nodes"]) + "predicted.csv")
        # plot the result
        image_output_filepath = config["parent_directory"] + "n_" + str(config["num_nodes"]) + "test_loss_by_time_comparison.png"
        if res_df is not None and gt_loss is not None:
            plot_result(res_df, 
                        config["num_nodes"], 
                        config["time_restriction"], 
                        gt_loss, 
                        filepath=image_output_filepath, 
                        )
