
from sklearn.base import BaseEstimator, ClassifierMixin
import numpy as np
from itertools import chain, combinations
import pyAgrum as gum
import pyAgrum.causal as csl
import time

"""
Graph Surgery Estimator Algorthm according to
Subbaswamy, Adarsh, Peter Schulam, and Suchi Saria. 
"Preventing failures due to dataset shift: Learning predictive models that transport." 
https://arxiv.org/abs/1812.04597

"""

class GraphSurgeryClassifier(BaseEstimator, ClassifierMixin):

    def __init__(self, graphmodel):
        
        self.cm_with_S = graphmodel.cm_with_S
        self.target = graphmodel.target
        self.cm_without_S = graphmodel.cm_without_S
        self.bn_with_latent_as_observed_without_S = graphmodel.bn_with_latent_as_observed_without_S
        self.SID = [self.cm_with_S.idFromName(s) for s in graphmodel.s_name_list]
        # get a dictionary converting id to variable names
        self.idToName = self.cm_with_S.names()
        self.nameToId_dict = dict((v,k) for k,v in self.cm_with_S.names().items()) 
         # get a list of children id
        ls_of_children_id = []
        for sid in self.SID:
            ls_of_children_id = ls_of_children_id + list(self.cm_with_S.children(sid))
        # get a list of children variable names
        ls_of_children = [self.idToName[i] for i in ls_of_children_id]
        # get children of S
        self.chS = ls_of_children 

        # get all variables names 
        ls_of_names_of_dag = list(self.cm_without_S.observationalBN().names())
        self.V = ls_of_names_of_dag[:]

        # get the induced subgraph of G_{\bar{Y}}
        bn_alt = gum.BayesNet(self.bn_with_latent_as_observed_without_S)
        yid = self.nameToId_dict[self.target]
        paY = bn_alt.parents(yid)
        for pa in list(paY):
            # remove the arc: pa -> y
            bn_alt.eraseArc(pa,yid) 
        self.G_bar_Y = bn_alt
        self.loss_min = np.inf
        self.id_result = None
        self.time_instances = []
        self.id_results = []
        self.train_losses = []
        self.intervention_set = None
        self.conditioning_set = None
        self.cpt = None
    
    def getSupersets(self, V, Y):
        """
            Get a superset of Y in V.

            Arg:

                V: all variables in G
                Y: a target set that is a subset of V
            Return:
                a generator that returns a list of variables as a superset of Y in V
        """
        for i in range(len(Y), len(V)+1):
            for subset in combinations(V, i):
                if set(Y).issubset(subset):
                    yield list(subset)

    def getPowerset(self, s):
        """
            Get the powerset of a set s
            
            Arg:
                s (list): a list of variables
            Return:
                a generator that gives a list of variable as powerset of s
        """
        n = len(s)
        for k in range(n+1):
            for combination in combinations(s, k):
                yield list(combination)


    def remove_elements(self, lst, elements_to_remove):
        """
            Remove a list of elements from another list.

            Arg:
                lst (list): a list of strings that represent a superset of "element_to_remove"
                elements_to_remove (list): a list of strings that denote which variable we want to remove from lst
        """
        for element in elements_to_remove:
            while element in lst:
                lst.remove(element)
        return lst
    
    def intersection(self, lst1, lst2):
        """
            take the intersection of two lists

            Arg:
                lst1(list): a list of strings
                lst2(list): a list of strings
        """
        return list(set(lst1) & set(lst2))


    def UQ(self, X, Y, Z, G):
            """
                converting conditional causal queries to unconditional causal queries for identification based on do-calulus

                Arg:
                    X (list): a list of strings that represent the intervention set
                    Y (str): a string that denotes the target
                    Z (list): a list of strings that represent the conditional set
                    G (PyAgrum Bayesnet object):  an object generated by PyAgrum BayesNet class
            """
            x_tilde = X[:] # intervention set
            y_tilde = [Y]
            if not Z:
                # check if Z is empty if so we immediately return
                return x_tilde, y_tilde
            z_tilde = Z[:] # conditioning set
            Zexists = True
            dummy_str = None
            lastupdate = False
            # copy the graph
            while Zexists:
                # update X' and Z'
                if dummy_str is not None:
                    x_tilde = x_tilde + [dummy_str]
                    z_tilde.remove(dummy_str)
                    dummy_str = None
                    
                # combine intervention and conditioning set in one list
                for counter, z in enumerate(z_tilde):
                    bn = gum.BayesNet(G)
                    # get id of z
                    zid = bn.idFromName(z)
                    # get all children of z
                    chz = list(bn.children(z))
                    # remove all outgoing edges from z to its children
                    for ch in chz:
                        # check if there exists
                        if bn.existsArc(zid,ch):
                            # if so, we remove it
                            bn.eraseArc(zid,ch)

                    # removing the incoming edges of X 
                    for x in x_tilde:
                        # for each member in the intervetnion set, get all of its parents
                        pax = list(bn.parents(x))
                        xid = bn.idFromName(x)
                        for pax_i in pax:
                            # if there is an arc from its pareant to 
                            if bn.existsArc(pax_i, xid):
                                bn.eraseArc(pax_i, xid)
                    proxy_z = z_tilde[:]
                    proxy_z.remove(z)
                    conditioning_set_for_testing_d_separation = x_tilde + proxy_z  
                    # Check for d-separation if Z is independent of y_tilde given the conditioning
                    bool_res = bn.isIndependent([z],y_tilde, conditioning_set_for_testing_d_separation)
                    if bool_res:
                        # if they are, we take a dummy variable
                        dummy_str = z
                        # if this is the last element in the condition
                        if counter == (len(z_tilde) - 1):
                            lastupdate = True
                            Zexists = False
                        break #  if independent we break out and update 
                    # if not and it is the last element in the Z, break out of the while loop by setting Z exists = False
                    if counter == (len(z_tilde) - 1):
                        Zexists = False
            if lastupdate:
                # Since we will break out of the while loop, we make the one last update
                x_tilde = x_tilde + [dummy_str]
                z_tilde.remove(dummy_str)
            y_tilde = y_tilde + z_tilde
            return x_tilde, y_tilde

    
    def predict(self, X, cpt= None):
        """
            predict 1/0 class based on the given conditional probability table and the given features
        """
        if cpt is None:
            cpt_df = self.cpt.putFirst(self.target).topandas()
        else:
            # convert the format to pandas dataframe
            cpt_df = cpt.putFirst(self.target).topandas()
        pred_list  = []
        # record our prediction for each combinations of discrete values
        for _, val in cpt_df.idxmax(axis=1):
            pred_list.append(int(val)) # select 0 or 1 based on the highest returned probabilitiy in the causal estimand
        X_copy = X.copy()
        X_copy.loc[:, self.target] = None
        multi_index_info = np.transpose(cpt_df).columns
        all_zero_ones_val_combinations = list(multi_index_info)
        features_to_select =  list(multi_index_info.names)
        for counter,i in enumerate(all_zero_ones_val_combinations):
            # get the combination and cast it as a list
            val_to_search = list(i)
            val_to_search = [int(k) for k in val_to_search]
            # filter the X test dataframe
            mask = (X_copy[features_to_select]==val_to_search).all(axis=1)
            # by default, the target column is the last column, we fill in the respective predicted values 
            X_copy.loc[mask, self.target] = pred_list[counter] 
        pred = X_copy[[self.target]]
        return pred

    def predict_proba(self, X ,cpt=None):
        """
            predict (in terms of probability) based on the given conditional probability table and the given features
        """
        if cpt is None:
            cpt_df = self.cpt.putFirst(self.target).topandas()
        else:
            # convert the format to pandas dataframe
            cpt_df = cpt.putFirst(self.target).topandas()
            
        pred_list = cpt_df[self.target]["1"].tolist() # get the probability of class 1
        X_copy = X.copy()
        X_copy.loc[:, self.target] = None
        multi_index_info = np.transpose(cpt_df).columns
        all_zero_ones_val_combinations = list( multi_index_info)
        features_to_select =  list(multi_index_info.names)
        for counter,i in enumerate(all_zero_ones_val_combinations):
            # get the combination and cast it as a list
            val_to_search = list(i)
            val_to_search = [int(k) for k in val_to_search]
            # filter the X test dataframe
            mask = (X_copy[features_to_select]==val_to_search).all(axis=1)
            # by default, the target column is the last column, we fill in the respective predicted values 
            X_copy.loc[mask, self.target] = pred_list[counter] 
        pred = X_copy[[self.target]]
        return pred
    
    def update(self, loss, id_result, conditioning_set, intervention_set, cpt, time_instance):
        """
            update the training parameters 
        """
        self.id_result = id_result
        self.loss_min = loss
        self.train_losses.append(loss)
        self.time_instances.append(time_instance)
        self.id_results.append(id_result)
        self.conditioning_set = conditioning_set
        self.intervention_set = intervention_set
        self.cpt = cpt
        print("Updated!")
        print("Intervention_set:{}".format(self.intervention_set))
        print("Conditioning_set:{}".format(self.conditioning_set))
        print("Training Loss:{}".format(self.loss_min))
        return self

    def eval(self, 
                 intervention_set, 
                 conditioning_set, 
                 X, 
                 y, 
                 loss_func, 
                 starttime):
        """
            given the found intervention and conditioning sets, 
            evaluate the causal query and compute the loss
        """
        id_result =  csl.doCalculusWithObservation(self.cm_without_S, 
                                                      on =set([self.target]), 
                                                      doing = set(intervention_set), 
                                                      knowing = set(conditioning_set))
        cpt = id_result.eval()
        # make sure we dont get P(Y)
        if cpt.topandas().size != 2:
            pred = self.predict(X, cpt)
            loss_current = loss_func(y, pred)
            if loss_current < self.loss_min:
                self.update(loss_current, 
                            id_result, 
                            conditioning_set, 
                            intervention_set, 
                            cpt, 
                            time.time() - starttime)
                
        return self

    def fit(self, X, y, loss_func, timelimit=None):
        """
            Graph Surgery Estimator fitting process
        """
        v_minus_y = self.V[:]
        v_minus_y.remove(self.target)
        # start the timer
        starttime = time.time()
        # Loop through the superset of children of S 
        for M in self.getSupersets(v_minus_y, self.chS):
            if timelimit is not None:
                currenttime = time.time() - starttime
                if currenttime > timelimit:
                    return self
            var_to_remove = M + [self.target]
            v = self.V[:]
            v_minus_m_and_y = self.remove_elements(v, var_to_remove)
            for Z in self.getPowerset(set(v_minus_m_and_y)):
                if timelimit is not None:
                    currenttime = time.time() - starttime
                    if currenttime > timelimit:
                        return self
                if self.target not in M:
                    try:
                        self.eval(M, Z, X, y, loss_func, starttime)
                    except:
                        # if we get a hedge, continue to the next conditioning set
                        continue
                # Run UQ algorithm 1
                X_list, Y_list = self.UQ(M, self.target, Z, self.G_bar_Y)
                # Add Y to X list and remove Y from Y_list
                Y_list.remove(self.target)
                X_list = X_list + [self.target]
                chY = self.G_bar_Y.children(self.target)
                # convert the children id to variable name
                chY_list = [self.idToName[chid] for chid in chY]
                T_list = [self.target] + chY_list
                # check if Y \cap (T \cup Ch(T)) = \emptyset
                res_list = self.intersection(Y_list, T_list)
                if not res_list:
                    # if so, we continue
                    continue
                try:
                    self.eval(X_list, Y_list, X, y, loss_func, starttime)
                except:
                    continue
        if self.loss_min < np.inf:
            print(f'final query:P({self.target}|do({self.intervention_set}), {self.conditioning_set})')
            print("GSE's corresponding training loss: {}".format(self.loss_min))
        else:
            raise Exception("GSE FAIL. There is no any graph surgery estimator.")
        return self

        

